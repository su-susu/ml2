# -*- coding: utf-8 -*-
"""
Created on Fri Dec 12 07:55:53 2017

@author: siye
"""
from sklearn import datasets
from sklearn import model_selection
import numpy as np
import matplotlib.pyplot as plt
import time

#定义Loss function Loss(w)=(||w||^2 )/2 + [C*(∑(max(0,1-(y-w.T*x+b)))) ]/n
def loss(X,y,w,C):
    m=y.shape[0]
    hinge = sum(list(map(lambda x:max(0,x[0]),(1-y.A*(X*w).A))))
    #计算 (||w||^2)/2  . 在目标函数中，max(1/||w||) 等价于 min((||w||^2)/2)
    w_2 = (sum(w.A**2)[0])/2    #gai
    return (w_2+ C*hinge)/m

def gradient(X,y,w,C):
    #设 ξ = (1- y*(w.T*A)) , gw(Xi) = dξ/dw 则gw(Xi)有（-yi*Xi) 和 0 两种取值
    m=y.shape[0]
    dw = np.zeros((X.shape[1],1))
    temp=(1-y.A*((X*w).A))
    #根据 temp>0 或 temp<0 对 dw 进行不同的操作
    for i in range(m):
        if temp[i]>=0:
            #d L(w,b）= w + ( C *∑(gw(x)))/n 
            dw+= w -  C*(y[i]*X[i]).T
        else:
            dw+= w
        
    return dw/m

#使用NAG 更新参数
def NAG_train(X,y,test_x,test_y,w,C,alpha,iteration):
    startTime=time.time()
    gamma=0.9       #设置γ一般取0.9就行了。直观上可以减少震荡，能更快的收敛。
    
    trainLoss=[]
    testLoss=[]
    trainLoss.append(loss(X,y,w,C)) #将最初的Loss加入到训练历史中
    testLoss.append(loss(test_x,test_y,w,C))
   
    vt = np.zeros(w.shape)
        
    for i in range(iteration):
        random = np.random.permutation(X.shape[0])[0:100]
        gdx = X[random]
        gdy = y[random]
        
        gt = gradient(gdx,gdy,w - gamma*vt,C)   #gt ← dL[w(t-1)-γ*v(t-1)]
        vt = gamma*vt + alpha*gt        #vt ← γ* v(t-1) + α*gt
        w = w - vt  # wt ← w(t-1) -vt

        trainLoss.append(loss(X,y,w,C))
        testLoss.append(loss(test_x,test_y,w,C))
        
    print('NAG completed! Took %f s!'%(time.time()-startTime))
    print("")     
    return w,trainLoss,testLoss

#使用RMSProp更新参数
def RMSProp_train(X,y,test_x,test_y,w,C,alpha,iteration):
    startTime=time.time()
    gamma=0.9
    e = 1e-8
    
    trainLoss=[]
    testLoss=[]
    trainLoss.append(loss(X,y,w,C))
    testLoss.append(loss(test_x,test_y,w,C))
    
    vt = np.zeros(w.shape)
    for i in range(iteration):
        random = np.random.permutation(X.shape[0])[0:100]
        gdx = X[random]
        gdy = y[random]
        
        gt = gradient(gdx,gdy,w,C)  # gt ← dL(w(t-1))
        vt = gamma*vt + (1-gamma)*(np.matrix(gt*gt)) # Gt ← γ * Gt + (1-γ)gt⊙gt
        w = w - 0.01/np.sqrt(vt+e).A*(gt)

        trainLoss.append(loss(X,y,w,C))
        testLoss.append(loss(test_x,test_y,w,C))
        
    print('RMSProp completed! Took %f s!'%(time.time()-startTime))
    print("")    
    return w,trainLoss,testLoss
    
#使用AdaDelta 更新参数
def AdaDelta_train(X,y,test_x,test_y,w,C,alpha,iteration):
    startTime=time.time()
    gamma = 0.95
    vt = np.zeros(w.shape)
    e = 1e-4
    tw = np.matrix(np.zeros(w.shape))
    
    trainLoss=[]
    testLoss=[]
    trainLoss.append(loss(X,y,w,C))
    testLoss.append(loss(test_x,test_y,w,C))
    
    for i in range(iteration):
        random = np.random.permutation(X.shape[0])[0:100]
        gdx = X[random]
        gdy = y[random]
        
        gt = gradient(gdx,gdy,w,C)
        vt = gamma*vt + (1-gamma)*(np.matrix((gt)**2))
        dw = -(np.sqrt(tw+e)/np.sqrt(vt+e)).A*gt
        w = w + dw
        tw = gamma*tw+(1-gamma)*(tw.A*tw.A)

        trainLoss.append(loss(X,y,w,C))
        testLoss.append(loss(test_x,test_y,w,C))
        
    print('AdaDelta completed! Took %f s!'%(time.time()-startTime))
    print("")  
    return w,trainLoss,testLoss
    
#使用Adam 更新参数
def Adam_train(X,y,test_x,test_y,w,C,alpha,iteration):
     startTime=time.time()
     r = 0.99
     vt = np.zeros(w.shape)
     e = 1e-6
     mw = np.zeros(w.shape)
     b1 = 0.9
    
     trainLoss = []
     testLoss = []
     trainLoss.append(loss(X,y,w,C))
     testLoss.append(loss(test_x,test_y,w,C))
    
     for i in range(iteration):
        random = np.random.permutation(X.shape[0])[0:100]
        gdx = X[random]
        gdy = y[random]
        
        gt = gradient(gdx,gdy,w,C)
        vt = r*vt + (1-r)*(np.matrix(gt**2))
        mw = b1*mw + (1-b1)*gt
        alp = alpha*np.sqrt(1-r)/(1-b1)
        w = w - alp*mw/np.sqrt(vt+e).A

        trainLoss.append(loss(X,y,w,C))
        testLoss.append(loss(test_x,test_y,w,C))
    
     print('Adam completed! Took %f s!'%(time.time()-startTime))
     print("") 
     return w,trainLoss,testLoss

def train(X,y,valid_x,valid_y):
    m = X.shape[1]
    init_w = np.matrix(np.random.randn(m,1))
    print("Begin to train")         #待删除
    C = 5
    alpha = 0.01    #设置学习率
    iteration=500
    
    w,train_loss_history,NAG_loss_history = NAG_train(X,y,valid_x,valid_y,init_w,C,alpha,iteration)
    w,train_loss_history,RMSProp_loss_history = RMSProp_train(X,y,valid_x,valid_y,init_w,C,alpha,iteration)
    w,train_loss_history,AdaDelta_loss_history= AdaDelta_train(X,y,valid_x,valid_y,init_w,C,alpha,iteration)
    w,train_loss_history,Adam_loss_history = Adam_train(X,y,valid_x,valid_y,init_w,C,alpha,iteration)


    plt.plot(np.arange(iteration+1),NAG_loss_history,label='NAG loss')
    plt.plot(np.arange(iteration+1),RMSProp_loss_history,label='RMSProp loss')
    plt.plot(np.arange(iteration+1),AdaDelta_loss_history,label='AdaDelta loss')
    plt.plot(np.arange(iteration+1),Adam_loss_history,label='Adam loss')
    plt.legend(loc=1)
    plt.xlabel('iteration')
    plt.ylabel('loss')
    return w

#获取数据集
def getData():
    X,y = datasets.load_svmlight_file('D:/MLExperiment/logisticRegression/a9a.txt',n_features=123)
    X = np.matrix(X.toarray())
    ones = np.matrix(np.ones((X.shape[0],1)))
    train_x = np.concatenate((ones,X),axis=1)
    train_y = np.matrix(y).T
    
    X,y = datasets.load_svmlight_file('D:/MLExperiment/logisticRegression/a9atest.txt',n_features=123)
    X = np.matrix(X.toarray())
    ones = np.matrix(np.ones((X.shape[0],1)))
    test_x = np.concatenate((ones,X),axis=1)
    test_y = np.matrix(y).T
    return train_x,test_x,train_y,test_y
    

train_x,test_x,train_y,test_y = getData()
w = train(train_x,train_y,test_x,test_y)
        

 