# -*- coding: utf-8 -*-
"""
Created on Wed Dec 13 16:05:39 2017

@author: siye
"""

from sklearn import datasets
from sklearn import model_selection
from sklearn import linear_model
import numpy as np
import matplotlib.pyplot as plt
import math
import jupyter
import random
import time

#sigmoid 函数
def sigmoid(Xi,theta):
    z=math.exp(Xi.dot(theta.T))
    return 1.0/(1+np.exp(-z))

#loss函数：  L(Θ)=∑[yi*log(h(xi))+(1-yi)*log(1-h(xi))]/(-m)
def loss(X,y,theta):
    m=y.shape[0]
    loss=0.
    for i in range(m):
        loss+=(y[i]*math.log(sigmoid(X[i,:],theta)))+((1-y[i])*math.log(1-sigmoid(X[i,:],theta)))
    loss /= - m
    return loss
    
#求导∂L/∂Θ = (1/m) ∑ (h(Xi) - yi) * Xi = (1/m) X.T (sigmoid(XΘ)-y)
def gradient(X,y,theta):
    m=y.shape[0]
    gradient=np.zeros(theta.shape)
    for i in range(m):
        gradient+=(sigmoid(X[i,:],theta)-y[i])*(X[i,:])
        
    gradient/=m
    return gradient

#使用NAG优化方法更新模型参数
def NAG_train(X, y, theta, learning_rate, gamma, iteration):
   startTime=time.time()           #计算训练时间
   test_loss_history = np.zeros((iteration, 1))
   v = np.zeros(theta.shape)
   for iter in range(iteration):
       index = random.randint(0, y.shape[0]-10)
       theta = theta - gamma * v
       v = gamma * v - learning_rate * gradient(X[index:index+1,:], y[index:index+1], theta)
       theta = theta + v
       test_loss_history[iter] = loss(Xt_, yt_, theta)[-1:]
       
   print ('NAG training is completed! Took %fs!'%(time.time()-startTime))
   return test_loss_history, theta
    
#使用RMSProp 更新模型参数
def RMSProp_train(X, y, theta, learning_rate, gamma, epsilon, iteration):
    startTime=time.time() 
    test_loss_history = np.zeros((iteration, 1))
    G_t = 0.
    Theta_gradient = np.zeros(theta.shape)
    for iter in range(iteration):
        index = random.randint(0, y.shape[0]-10)
        Theta_gradient = gradient(X[index:index+10,:], y[index:index+10], theta)
        G_t = gamma * G_t + (1 - gamma) * Theta_gradient.dot(Theta_gradient.T)
        theta = theta - (learning_rate / np.sqrt(G_t + epsilon)) * Theta_gradient
        test_loss_history[iter] = loss(Xt_, yt_, theta)[-1:]
         
    print ('RMSProp training is completed! Took %fs!'%(time.time()-startTime))
    return test_loss_history, theta

#使用AdaDelta 更新模型
def AdaDelta_train(X, y, theta, gamma, epsilon, iteration):
    startTime=time.time()
    test_loss_history = np.zeros((iteration, 1))
    Theta_gradient = np.zeros(theta.shape)
    G_t = 0.
    delta_theta = np.zeros(theta.shape)
    delta_t = 0.03
    for iter in range(iteration):
        index = random.randint(0, y.shape[0]-10)
        Theta_gradient = gradient(X[index:index+10,:], y[index:index+10], theta)
        G_t = gamma * G_t + (1 - gamma) * Theta_gradient.dot(Theta_gradient.T)
        delta_theta = - (np.sqrt(delta_t + epsilon) / np.sqrt(G_t + epsilon)) * Theta_gradient
        theta = theta + delta_theta
        delta_t = gamma * delta_t + (1 - gamma) * (delta_theta.dot(delta_theta.T))
        test_loss_history[iter] = loss(Xt_, yt_, theta)[-1:]
    print('AdaDelta training is completed! Took %f s!'%(time.time()-startTime))
    return test_loss_history, theta

#使用Adam 更新模型
def Adam_train(X, y, theta, learning_rate, beta1, beta2, epsilon, iteration):
    startTime=time.time()
    test_loss_history = np.zeros((iteration, 1))
    Theta_gradient = np.zeros(theta.shape)
    v_t = 0.
    m_t = np.zeros(theta.shape)
    for iter in range(iteration):
        index = random.randint(0, y.shape[0]-10)
        Theta_gradient = gradient(X[index:index+10,:], y[index:index+10], theta)
        m_t = beta1 * m_t + (1 - beta1) * Theta_gradient
        v_t = beta2 * v_t + (1 - beta2) * Theta_gradient.dot(Theta_gradient.T)
        mt_estimate = m_t / (1 - pow(beta1, iter + 1))
        vt_estimate = v_t / (1 - pow(beta2, iter + 1))
        theta = theta - learning_rate * mt_estimate / (np.sqrt(vt_estimate) + epsilon)	
        test_loss_history[iter] = loss(Xt_, yt_, theta)[-1:]    
    print('Adam training is completed! Took %f s!'%(time.time()-startTime))
    return test_loss_history, theta


#读取训练集和验证集
X_train, y_train = datasets.load_svmlight_file("D:/MLExperiment/logisticRegression/a9a.txt")

x_ = np.array(X_train.toarray(), np.float32).reshape((-1, 123))
y_ = np.array(y_train, np.float32).reshape((-1, 1))

for i in range(y_.shape[0]):
	if y_[i,0] == -1.0 : y_[i,0] = 0.

X_ = np.hstack([x_, np.ones((x_.shape[0], 1))])

X_test, y_test = datasets.load_svmlight_file("D:/MLExperiment/logisticRegression/a9atest.txt")
xt_ = np.array(X_test.toarray(), np.float32).reshape((-1, 122))
yt_ = np.array(y_test, np.float32).reshape((-1, 1))
for i in range(yt_.shape[0]):
	if yt_[i,0] == -1.0 : yt_[i,0] = 0.
    
Xt_ = np.hstack([xt_, np.zeros((xt_.shape[0], 1)),np.ones((xt_.shape[0], 1))])


#设置迭代次数
iteration=500

#超参数设置
be1 = 0.9
be2 = 0.999
ep = 1e-8

t_nag = np.zeros((1, 124))
t_rmsprop = np.zeros((1, 124))
t_adadelta = np.zeros((1, 124))
t_adam = np.zeros((1, 124))


#NAG
nag_loss_history, t_nag = NAG_train(X_, y_, t_nag, 0.005, be1, iteration)
#RMSProp
rmsprop_loss_history, t_rmsprop = RMSProp_train(X_, y_, t_rmsprop, 0.005, be1, ep, iteration)
#Adadelta
adadelta_loss_history, t_adadelta = AdaDelta_train(X_, y_, t_adadelta, be1, ep, iteration)
#Adam
adam_loss_history, t_adam = Adam_train(X_, y_, t_adam, 0.005, 0.999, 0.99, ep, iteration)

#绘制graph
plt.plot(nag_loss_history, 'g', label='NAG loss')
plt.plot(rmsprop_loss_history, 'b', label='RMSProp loss')
plt.plot(adadelta_loss_history, 'r', label='AdaDelta loss')
plt.plot(adam_loss_history, 'y', label='Adam loss')
plt.legend(loc='upper right')
plt.ylabel('loss');
plt.xlabel('iteration ')
plt.show()